{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tratamento dos dados\n",
    "Nesta etapa será realizado todo o tratamento dos dados, como fazer as transformações de tipo de variável, tratamento de missing, tratamento de outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T14:38:06.841056Z",
     "start_time": "2023-04-10T14:38:05.831562Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T14:38:06.954201Z",
     "start_time": "2023-04-10T14:38:06.851083Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importando os dados\n",
    "df_vendas = pd.read_csv('../data/raw/sales.csv', index_col=0)\n",
    "df_estoque_lvl = pd.read_csv('../data/raw/sensor_stock_levels.csv', index_col=0)\n",
    "df_temperatura = pd.read_csv('../data/raw/sensor_storage_temperature.csv', index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tratando o timestamp\n",
    "Conforme visto na documentação, as tabelas podem ser unidas usando o timestamp, mas essa variável é medida de forma diferente nas tabelas, dificultando assim a mesclagem. Para resolver isso irei alterar o timestamp apenas para data e hora sem minutos, pois como definido no problema, os sensores realizarão medidas hora a hora.\n",
    "\n",
    "Como a transformação funcionará:\n",
    "\n",
    "06/05/2020 19:45:26 -> 06/05/2020 19:00:00\n",
    "<br>\n",
    "09/04/2019 16:10:08 -> 09/04/2019 16:00:00\n",
    "<br>\n",
    "19/10/2021 09:30:21 -> 19/10/2021 09:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T14:34:31.997776Z",
     "start_time": "2023-04-10T14:34:31.938382Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alterando a coluna timestamp\n",
    "df_vendas['timestamp'] = pd.to_datetime(df_vendas.timestamp.str.slice(0, 13))\n",
    "df_estoque_lvl['timestamp'] = pd.to_datetime(df_estoque_lvl.timestamp.str.slice(0, 13))\n",
    "df_temperatura['timestamp'] = pd.to_datetime(df_temperatura.timestamp.str.slice(0, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Unindo as tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T14:34:32.077008Z",
     "start_time": "2023-04-10T14:34:31.997776Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5639, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unindo as tabelas e verificando o shape\n",
    "df_vendas.merge(df_estoque_lvl, on = ['product_id', 'timestamp']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T14:34:32.092532Z",
     "start_time": "2023-04-10T14:34:32.028520Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27972921190445776"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculando a perda de dados\n",
    "1 - df_vendas.merge(df_estoque_lvl, on = ['product_id', 'timestamp']).shape[0]/df_vendas.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Como a data e hora das vendas são diferentes das datas e hora das medições dos sensores, a união realizada pelo método **merge** está causando uma perda de cerca de 27% dos dados de vendas, o que é bastante.\n",
    "\n",
    "Para solucionar esse problema, iremos unir os datasets manualmente utilizando o id e data, onde cada item será unido ao percentual de estoque de mesmo timestamp da venda, onde o timestamp anterior à venda será utilizado caso a união anterior não seja feita.\n",
    "\n",
    "Além disso, como as predições serão feitas de hora em hora, adicionarei o percentual de estoque posterior ao da venda, onde tal dado será o nosso **target**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando listas vazias\n",
    "lista_prcnt_estoque_futuro = []\n",
    "lista_prcnt_estoque_passado = []\n",
    "\n",
    "# Criando um loop para iterar sob cada indice\n",
    "for num in range(0, df_vendas.shape[0]):\n",
    "    \n",
    "    # Obtendo o id do produto e o timestamp da venda\n",
    "    timestamp_venda = df_vendas.iloc[num, 1]\n",
    "    id_produto = df_vendas.iloc[num, 2]\n",
    "    \n",
    "    # Encontrando todos os registros de estoque do produto e adicionando o timestamp de venda a ela\n",
    "    lista_filtrada = df_estoque_lvl.query(f\"product_id == '{id_produto}'\").timestamp.to_list()\n",
    "    lista_filtrada_original = lista_filtrada.copy()\n",
    "    lista_filtrada.append(timestamp_venda)\n",
    "    \n",
    "    # Removendo duplicatas e ordenando a lista \n",
    "    lista_sem_duplicatas = set(lista_filtrada)\n",
    "    lista_ordenada = sorted(lista_sem_duplicatas)\n",
    "    \n",
    "    # Verificando se o timestamp está na lista\n",
    "    if timestamp_venda in lista_filtrada_original:\n",
    "        \n",
    "        # Se sim, busca o menor lvl de estoque registrado naquela hora\n",
    "        registro = df_estoque_lvl.query(f\"product_id == '{id_produto}' and timestamp == '{timestamp_venda}'\")\n",
    "        registro_ordenado = registro.sort_values('estimated_stock_pct')\n",
    "        lvl_estoque_passado = registro.estimated_stock_pct.values[0]\n",
    "        lista_prcnt_estoque_passado.append(lvl_estoque_passado)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # Se não, busca o lvl de estoque do horário passado mais próximo do atual\n",
    "        try:\n",
    "            index_novo = lista_ordenada.index(timestamp_venda) - 1\n",
    "            if index_novo < 0:\n",
    "                lista_prcnt_estoque_passado.append(np.nan)\n",
    "            else:\n",
    "                timestamp_passado = lista_ordenada[index_novo]\n",
    "                registro = df_estoque_lvl.query(f\"product_id == '{id_produto}' and timestamp == '{timestamp_passado}'\")\n",
    "                registro_ordenado = registro.sort_values('estimated_stock_pct')\n",
    "                lvl_estoque_passado = registro.estimated_stock_pct.values[0]\n",
    "                lista_prcnt_estoque_passado.append(lvl_estoque_passado)\n",
    "        \n",
    "        except:\n",
    "            lista_prcnt_estoque_passado.append(np.nan)\n",
    "    \n",
    "    # Busca o lvl de estoque da hora seguinte    \n",
    "    try:    \n",
    "        index_novo = lista_ordenada.index(timestamp_venda) + 1\n",
    "        if index_novo > len(lista_ordenada):\n",
    "            lista_prcnt_estoque_futuro.append(np.nan)\n",
    "        else:\n",
    "            timestamp_passado = lista_ordenada[index_novo]\n",
    "            timestamp_futuro = lista_ordenada[lista_ordenada.index(timestamp_venda) + 1]\n",
    "            registro = df_estoque_lvl.query(f\"product_id == '{id_produto}' and timestamp == '{timestamp_futuro}'\")\n",
    "            registro_ordenado = registro.sort_values('estimated_stock_pct')\n",
    "            lvl_estoque_futuro = registro.estimated_stock_pct.values[0]\n",
    "            lista_prcnt_estoque_futuro.append(lvl_estoque_futuro)\n",
    "        \n",
    "    except:\n",
    "        lista_prcnt_estoque_futuro.append(np.nan)\n",
    "        \n",
    "# Adiciona os valores a tabela\n",
    "df_vendas['lvl_estoque_past'] = lista_prcnt_estoque_passado\n",
    "df_vendas['lvl_estoque_to_predict'] = lista_prcnt_estoque_futuro    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Aqui a nossa abordagem será alterada, pois ao contrário da tabela de estoque, onde tínhamos valores únicos por timestamp devido a diferenciação proporcionada ID, não possuímos a mesma diferenciação aqui, onde cada timestamp possui vários valores diferentes de temperatura associados a ele.\n",
    "\n",
    "Sendo assim, irei agrupar o timestamp e usarei a coluna de temperatura par calcular algumas estatísticas para cada horário, como média, mediana, etc. É necessário valores únicos para uma união sem perda de valor entre as tabelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:02.844542Z",
     "start_time": "2023-04-08T17:58:02.720188Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computando estatísticas e resetando o index\n",
    "df_temperatura = df_temperatura.groupby('timestamp').temperature.agg(['mean', 'median', 'min', 'max', 'std', 'var'])\n",
    "df_temperatura = df_temperatura.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:02.908702Z",
     "start_time": "2023-04-08T17:58:02.761043Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unindo as  e salvando os dados\n",
    "df_unido = df_vendas.merge(df_temperatura, on = 'timestamp')\n",
    "df_unido.to_csv('../data/interim/dados_unidos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:02.920227Z",
     "start_time": "2023-04-08T17:58:02.892301Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>quantity</th>\n",
       "      <th>total</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>lvl_estoque_past</th>\n",
       "      <th>lvl_estoque_to_predict</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1c82654-c52c-45b3-8ce8-4c2a1efe63ed</td>\n",
       "      <td>2022-03-02 09:00:00</td>\n",
       "      <td>3bc6c1ea-0198-46de-9ffd-514ae3338713</td>\n",
       "      <td>fruit</td>\n",
       "      <td>gold</td>\n",
       "      <td>3.99</td>\n",
       "      <td>2</td>\n",
       "      <td>7.98</td>\n",
       "      <td>e-wallet</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.6729</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-30.58</td>\n",
       "      <td>34.33</td>\n",
       "      <td>11.456305</td>\n",
       "      <td>131.246935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c7efb312-1956-4185-875a-cd6c3714e127</td>\n",
       "      <td>2022-03-02 09:00:00</td>\n",
       "      <td>3bc6c1ea-0198-46de-9ffd-514ae3338713</td>\n",
       "      <td>fruit</td>\n",
       "      <td>premium</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3</td>\n",
       "      <td>11.97</td>\n",
       "      <td>credit card</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.6729</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-30.58</td>\n",
       "      <td>34.33</td>\n",
       "      <td>11.456305</td>\n",
       "      <td>131.246935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>514ea59d-3f6b-42e4-9b48-eb7a45126361</td>\n",
       "      <td>2022-03-02 09:00:00</td>\n",
       "      <td>14736243-d346-438f-9535-d80fcb9f3882</td>\n",
       "      <td>fruit</td>\n",
       "      <td>premium</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4</td>\n",
       "      <td>5.96</td>\n",
       "      <td>e-wallet</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.6729</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-30.58</td>\n",
       "      <td>34.33</td>\n",
       "      <td>11.456305</td>\n",
       "      <td>131.246935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34ba9d8c-bd75-4533-920d-dd5164865305</td>\n",
       "      <td>2022-03-02 09:00:00</td>\n",
       "      <td>ad81b46c-bf38-41cf-9b54-5fe7f5eba93e</td>\n",
       "      <td>fruit</td>\n",
       "      <td>non-member</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3</td>\n",
       "      <td>11.97</td>\n",
       "      <td>cash</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.6729</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-30.58</td>\n",
       "      <td>34.33</td>\n",
       "      <td>11.456305</td>\n",
       "      <td>131.246935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ba2b8370-9980-49cd-be8d-a2a974f05a96</td>\n",
       "      <td>2022-03-02 09:00:00</td>\n",
       "      <td>7f5e86e6-f06f-45f6-bf44-27b095c9ad1d</td>\n",
       "      <td>fruit</td>\n",
       "      <td>premium</td>\n",
       "      <td>4.49</td>\n",
       "      <td>2</td>\n",
       "      <td>8.98</td>\n",
       "      <td>credit card</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.6729</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>-30.58</td>\n",
       "      <td>34.33</td>\n",
       "      <td>11.456305</td>\n",
       "      <td>131.246935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         transaction_id           timestamp  \\\n",
       "0  a1c82654-c52c-45b3-8ce8-4c2a1efe63ed 2022-03-02 09:00:00   \n",
       "1  c7efb312-1956-4185-875a-cd6c3714e127 2022-03-02 09:00:00   \n",
       "2  514ea59d-3f6b-42e4-9b48-eb7a45126361 2022-03-02 09:00:00   \n",
       "3  34ba9d8c-bd75-4533-920d-dd5164865305 2022-03-02 09:00:00   \n",
       "4  ba2b8370-9980-49cd-be8d-a2a974f05a96 2022-03-02 09:00:00   \n",
       "\n",
       "                             product_id category customer_type  unit_price  \\\n",
       "0  3bc6c1ea-0198-46de-9ffd-514ae3338713    fruit          gold        3.99   \n",
       "1  3bc6c1ea-0198-46de-9ffd-514ae3338713    fruit       premium        3.99   \n",
       "2  14736243-d346-438f-9535-d80fcb9f3882    fruit       premium        1.49   \n",
       "3  ad81b46c-bf38-41cf-9b54-5fe7f5eba93e    fruit    non-member        3.99   \n",
       "4  7f5e86e6-f06f-45f6-bf44-27b095c9ad1d    fruit       premium        4.49   \n",
       "\n",
       "   quantity  total payment_type  lvl_estoque_past  lvl_estoque_to_predict  \\\n",
       "0         2   7.98     e-wallet              0.37                    0.48   \n",
       "1         3  11.97  credit card              0.37                    0.48   \n",
       "2         4   5.96     e-wallet              0.54                    0.33   \n",
       "3         3  11.97         cash              0.51                    0.04   \n",
       "4         2   8.98  credit card              0.71                    0.98   \n",
       "\n",
       "     mean  median    min    max        std         var  \n",
       "0 -0.6729   -1.26 -30.58  34.33  11.456305  131.246935  \n",
       "1 -0.6729   -1.26 -30.58  34.33  11.456305  131.246935  \n",
       "2 -0.6729   -1.26 -30.58  34.33  11.456305  131.246935  \n",
       "3 -0.6729   -1.26 -30.58  34.33  11.456305  131.246935  \n",
       "4 -0.6729   -1.26 -30.58  34.33  11.456305  131.246935  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checando o resultado da união\n",
    "df_unido.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Criando novas features\n",
    "Agora que já possuímos um dataset completo, vou criar mais algumas feature a partir dos dados de tempo e dropar os id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:02.993812Z",
     "start_time": "2023-04-08T17:58:02.920227Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Buscando o dia da semana\n",
    "df_unido['day_of_week'] = df_unido.timestamp.dt.weekday\n",
    "\n",
    "# Buscando se é final de semana\n",
    "df_unido['is_weekend'] = df_unido.day_of_week.apply(lambda day: 'yes' if day > 4 else 'no')\n",
    "\n",
    "# Buscando a hora e turno\n",
    "df_unido['hour'] = df_unido.timestamp.dt.hour\n",
    "df_unido['turn'] = df_unido.hour.apply(lambda hour: 'morning' if hour < 12 else ('afternoon' if 12 <= hour < 18 else 'night'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Investigando Outliers\n",
    "Como vimos na etapa de EDA, existem alguns outliers nas colunas **total** e **unit_price**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:03.047268Z",
     "start_time": "2023-04-08T17:58:02.969128Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculando os quartis e o IQR\n",
    "q1 = np.quantile(df_unido.total, 0.25)\n",
    "q3 = np.quantile(df_unido.total, 0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "#Separando os dados sem e com outliers\n",
    "dados_com_outliers = df_unido.query(f\"total > {q3 + 1.5 * iqr}\")\n",
    "dados_sem_outliers = df_unido.query(f\"total < {q3 + 1.5 * iqr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:03.047268Z",
     "start_time": "2023-04-08T17:58:03.008110Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verificando a quantidade\n",
    "dados_com_outliers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:03.354956Z",
     "start_time": "2023-04-08T17:58:03.029527Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verificando as distribuições\n",
    "plt.hist(dados_com_outliers.total, alpha = 0.75)\n",
    "plt.hist(dados_sem_outliers.total, alpha = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " A distribuição dos dados é relativamente próxima, não existindo outliers discrepantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:03.480907Z",
     "start_time": "2023-04-08T17:58:03.340356Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"O preço médio por unidade dos dados sem outliers é de: {dados_sem_outliers.unit_price.mean()}\")\n",
    "print(f\"O preço médio por unidade dos dados com outliers é de: {dados_com_outliers.unit_price.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " diferença no preço é relativamente alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:03.480907Z",
     "start_time": "2023-04-08T17:58:03.388751Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"A quantidade média de itens comprados nos dados sem outliers é de: {dados_sem_outliers.quantity.mean()}\")\n",
    "print(f\"A quantidade média de itens comprados nos dados com outliers é de: {dados_com_outliers.quantity.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A quantidade de itens comprados também possui uma diferença significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:03.480907Z",
     "start_time": "2023-04-08T17:58:03.405275Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculando os quartis e o IQR\n",
    "q1 = np.quantile(df_unido.unit_price, 0.25)\n",
    "q3 = np.quantile(df_unido.unit_price, 0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "#Separando os dados sem e com outliers\n",
    "dados_com_outliers = df_unido.query(f\"unit_price > {q3 + 1.5 * iqr}\")\n",
    "dados_sem_outliers = df_unido.query(f\"unit_price < {q3 + 1.5 * iqr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:03.497424Z",
     "start_time": "2023-04-08T17:58:03.433610Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verificando a quantidade\n",
    "dados_com_outliers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:03.704312Z",
     "start_time": "2023-04-08T17:58:03.451655Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verificando as distribuições\n",
    "plt.hist(dados_com_outliers.unit_price, alpha = 0.75)\n",
    "plt.hist(dados_sem_outliers.unit_price, alpha = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Levando em consideração que as unidades mais caras estão bem próximas das não consideradas outliers, e que compras com outliers são aquelas que possuem os produtos mais caros e que são levados em uma quantidade próxima ao máximo registrado, posso concluir que os outliers são naturais. Removê-los ou tratar como uma inconsistência atrapalharia no aprendizado do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dropando colunas\n",
    "Nesta etapa irei deletar algumas colunas que não agregam valor, como id's e timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T17:58:03.721858Z",
     "start_time": "2023-04-08T17:58:03.704312Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Excluindo as colunas\n",
    "df_unido = df_unido.drop(columns=['transaction_id', 'product_id', 'timestamp'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Levando em conta que não temos mais mais tratamentos para aplicar, não possuímos valores missing e nem duplicatas ou inconsistências, irei utilizar a última versão do dataset para construir um modelo de Machine Learning. Entretanto, como novas variáveis foram criadas, irei checar as relações e correlações em busca de insights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checando correlações e relacionamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando as relações entre variáveis numéricas\n",
    "sns.pairplot(df_unido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando as correlações\n",
    "sns.heatmap(df_unido.corr(numeric_only=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checando valores nulos\n",
    "Já era esperado que dados ausentes fossem produzidos a partir da união das tabelas, pois vendas que ocorreram na primera hora do primeiro dia não possuem dados de estoque anteriores a essa data, além da não disponibilidade de dados target para serem utilizados nas vendas do ultimo dia durante o ultimo horário.\n",
    "\n",
    "A partir disso, irei dropar os dados que possuem nulo no target. As variáveis explicativas que possuem missing serão mantidas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropando os registros\n",
    "df_unido = df_unido.dropna(subset='lvl_estoque_to_predict')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando dados finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando os dados\n",
    "df_unido.to_csv('../data/interim/dados_para_treino.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
